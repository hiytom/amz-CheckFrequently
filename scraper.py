import asyncio
import logging
from playwright.async_api import async_playwright  # å¯¼å…¥ Playwright çš„å¼‚æ­¥ API
import json
import random
import pandas as pd  # ç”¨äºå°†æ•°æ®ä¿å­˜ä¸º CSV
import time
import re  # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼å¤„ç†

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("crawler.log", encoding="utf-8")
    ]
)

# å®šä¹‰å¸¸é‡
COOKIES_FILE = "amazon_cookies.json"  # Cookies æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ¨¡æ‹Ÿç™»å½•
OUTPUT_FILE = "amazon_products.csv"  # æµ‹è¯•æ¨¡å¼ä¸‹ä¿å­˜ç»“æœçš„ CSV æ–‡ä»¶å
MAX_RETRIES = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå¤„ç†æŠ“å–å¤±è´¥çš„æƒ…å†µ

# è·å–å•†å“å˜ä½“ ASIN çš„è¾…åŠ©å‡½æ•°
async def get_variants_asins(page):
    """
    ä»å•†å“è¯¦æƒ…é¡µæå–æ‰€æœ‰å˜ä½“ ASINï¼ˆå¦‚é¢œè‰²ã€å°ºå¯¸ç­‰å˜ä½“çš„ ASINï¼‰ã€‚
    
    :param page: Playwright é¡µé¢å¯¹è±¡
    :return: listï¼ŒåŒ…å«å˜ä½“ ASIN çš„åˆ—è¡¨
    """
    variant_asins = set()  # ä½¿ç”¨é›†åˆé¿å…é‡å¤
    # æŸ¥æ‰¾åŒ…å«å˜ä½“ ASIN çš„å…ƒç´ ï¼Œå¯èƒ½å‡ºç°åœ¨ä¸åŒæ ‡ç­¾ä¸­
    variant_elements = await page.query_selector_all("li[data-asin], div[data-defaultasin], div[data-csa-c-asin]")
    for elem in variant_elements:
        # ä»ä¸åŒå±æ€§ä¸­æå– ASIN
        asin = await elem.get_attribute("data-asin") or await elem.get_attribute("data-defaultasin") or await elem.get_attribute("data-csa-c-asin")
        if asin:  # å¦‚æœæ‰¾åˆ° ASINï¼Œæ·»åŠ åˆ°é›†åˆ
            variant_asins.add(asin.strip())
    return list(variant_asins)  # è¿”å›å»é‡åçš„åˆ—è¡¨

# æ ¸å¿ƒå‡½æ•°ï¼ŒæŠ“å–å•ä¸ªå•†å“çš„è¯¦æƒ…
async def get_product_details(asin, page, retry_count=0):
    """
    ä» Amazon å•†å“é¡µé¢æŠ“å–è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æ ‡é¢˜ã€å“ç‰Œã€ä»·æ ¼ç­‰ï¼‰ã€‚
    
    :param asin: strï¼Œå•†å“çš„ ASIN
    :param page: Playwright é¡µé¢å¯¹è±¡
    :param retry_count: intï¼Œå½“å‰é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 0
    :return: dictï¼ŒåŒ…å«å•†å“è¯¦æƒ…ï¼›è‹¥å¤±è´¥æˆ–éè¯¦æƒ…é¡µï¼Œè¿”å› None
    """
    url = f"https://www.amazon.com/dp/{asin}"  # æ„é€ å•†å“è¯¦æƒ…é¡µ URL
    logging.info(f"ğŸ“¦ æ­£åœ¨çˆ¬å–å•†å“è¯¦æƒ…: {url}")
    start_time = time.perf_counter()  # è®°å½•å¼€å§‹æ—¶é—´
    try:
        # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œé™ä½åçˆ¬é£é™©
        await asyncio.sleep(random.uniform(0.2, 0.8))
        # å®šä¹‰å¸¸è§çš„ User-Agentï¼Œä¼ªè£…ä¸ºçœŸå®æµè§ˆå™¨
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0"
        ]
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œéšæœºé€‰æ‹© User-Agent
        await page.set_extra_http_headers({"User-Agent": random.choice(user_agents)})
        # è®¿é—®å•†å“é¡µé¢ï¼Œç­‰å¾… DOM åŠ è½½å®Œæˆ
        await page.goto(url, timeout=60000, wait_until="domcontentloaded")

        # æ£€æŸ¥æ˜¯å¦ä¸ºå•†å“è¯¦æƒ…é¡µ
        title_element = await page.query_selector("#productTitle")  # å•†å“æ ‡é¢˜å…ƒç´ 
        price_element = await page.query_selector("span.a-price") or await page.query_selector("span.a-offscreen")  # ä»·æ ¼å…ƒç´ 
        if not title_element and not price_element:  # å¦‚æœæ ‡é¢˜å’Œä»·æ ¼éƒ½ä¸å­˜åœ¨ï¼Œè®¤ä¸ºæ˜¯éè¯¦æƒ…é¡µ
            content = await page.content()
            logging.warning(f"âš ï¸ ASIN {asin} ä¸æ˜¯å•†å“è¯¦æƒ…é¡µï¼Œè·³è¿‡çˆ¬å–ã€‚é¡µé¢å†…å®¹: {content[:500]}")
            return None

        # æ£€æŸ¥æ˜¯å¦é‡åˆ°éªŒè¯ç 
        captcha = await page.query_selector("input#captchacharacters")
        if captcha:
            logging.warning(f"âŒ ASIN {asin} é‡åˆ°éªŒè¯ç ï¼Œæš‚åœç­‰å¾…æ‰‹åŠ¨è§£å†³...")
            await asyncio.sleep(60)  # æš‚åœ 60 ç§’ï¼Œç­‰å¾…æ‰‹åŠ¨è§£å†³
            await page.reload()  # é‡æ–°åŠ è½½é¡µé¢

        # ç­‰å¾…æ ‡é¢˜å…ƒç´ åŠ è½½ï¼Œç¡®ä¿é¡µé¢å®Œå…¨å¯ç”¨
        await page.wait_for_selector("#productTitle", timeout=90000)
        title = await title_element.inner_text() if title_element else "Title not found"  # è·å–æ ‡é¢˜

        # è·å–å“ç‰Œä¿¡æ¯
        brand_element = await page.query_selector("#bylineInfo")
        brand = await brand_element.inner_text() if brand_element else "Brand not found"  # è·å–å“ç‰Œæ–‡å­—
        brand_link = await brand_element.get_attribute("href") if brand_element else None  # è·å–å“ç‰Œé“¾æ¥
        if "Visit the" in brand and "Store" in brand:  # æ¸…ç†å“ç‰Œåç§°ä¸­çš„å†—ä½™éƒ¨åˆ†
            brand = brand.replace("Visit the", "").replace("Store", "").strip()
        elif "Brand:" in brand:
            brand = brand.replace("Brand:", "").strip()
        brand = re.sub(r'[^a-zA-Z0-9\s-]', '', brand).strip()  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        if brand_link and not brand_link.startswith("http"):  # è¡¥å…¨å“ç‰Œé“¾æ¥
            brand_link = f"https://www.amazon.com{brand_link}"

        # æå–ä»·æ ¼
        price = "Price not found"
        if price_element:
            price_text = await price_element.inner_text()
            price_match = re.search(r'\$\d+\.\d{2}', price_text)  # æå–ç¬¬ä¸€ä¸ªåˆæ³•ä»·æ ¼
            if price_match:
                price = price_match.group(0)
            else:
                price_text = re.sub(r'[\n\r\s]+', '', price_text.strip())  # æ¸…ç†æ–‡æœ¬
                if price_text:
                    price = price_text
        else:
            # å¦‚æœæœªæ‰¾åˆ°å®Œæ•´ä»·æ ¼ï¼Œå°è¯•æ‹¼æ¥æ•´æ•°å’Œå°æ•°éƒ¨åˆ†
            price_whole_element = await page.query_selector("span.a-price-whole")
            price_fraction_element = await page.query_selector("span.a-price-fraction")
            whole_text = (await price_whole_element.inner_text()).strip() if price_whole_element else ""
            fraction_text = (await price_fraction_element.inner_text()).strip() if price_fraction_element else ""
            whole_text = re.sub(r'[\n\r\s.]', '', whole_text)  # æ¸…ç†æ•´æ•°éƒ¨åˆ†
            fraction_text = re.sub(r'[\n\r\s]', '', fraction_text)  # æ¸…ç†å°æ•°éƒ¨åˆ†
            if whole_text and fraction_text:
                price = f"${whole_text}.{fraction_text}"
            elif whole_text:
                price = f"${whole_text}"

        # è·å–ä¸Šæœˆé”€é‡
        bought_element = await page.query_selector("#social-proofing-faceout-title-tk_bought .a-text-bold")
        bought = (await bought_element.inner_text()).split()[0] if bought_element else "< 50"

        # è·å–é¢æ–™ç±»å‹
        fabric_type = None
        details_section = await page.query_selector("#productFactsDesktopExpander")
        if details_section:
            fabric_element = await details_section.query_selector("span.a-color-base:has-text('Fabric type')")
            if fabric_element:
                fabric_type_element = await fabric_element.evaluate_handle(
                    "el => el.parentElement.parentElement.nextElementSibling.querySelector('.a-color-base')"
                )
                fabric_type = await fabric_type_element.inner_text() if fabric_type_element else None

        # æ£€æŸ¥æ˜¯å¦ä¸ºâ€œç»å¸¸é€€è´§â€å•†å“
        frequently_returned_element = await page.query_selector(
            "div#buyingOptionNostosBadge_feature_div .hrrv-badge-T2-title p span.a-text-bold"
        )
        frequently_returned = True if frequently_returned_element else False

        # è·å–å˜ä½“ ASIN
        variant_asins = await get_variants_asins(page)

        # è·å–è¯„åˆ†å’Œè¯„è®ºæ•°
        rating = "Rating not found"
        review_count = "Review count not found"
        rating_element = await page.query_selector("#averageCustomerReviews .a-icon-alt")
        review_count_element = await page.query_selector("#acrCustomerReviewText")
        if rating_element:
            rating_text = await rating_element.inner_text()
            rating_match = re.search(r"(\d+\.\d+|\d+)", rating_text)  # æå–è¯„åˆ†æ•°å­—
            rating = rating_match.group(0) if rating_match else "Rating not found"
        if review_count_element:
            review_text = await review_count_element.inner_text()
            review_match = re.search(r"(\d+,?\d*)", review_text)  # æå–è¯„è®ºæ•°
            review_count = review_match.group(0).replace(",", "") if review_match else "Review count not found"

        # è·å–â€œå®¢æˆ·è¯„ä»·â€æ€»ç»“
        customer_say = "Customer say not found"
        insights_section = await page.query_selector("#cr-product-insights-cards")
        if insights_section:
            summary_element = await insights_section.query_selector("#product-summary p span")
            if summary_element:
                customer_say = await summary_element.inner_text()

        # è·å–è´Ÿé¢åé¦ˆè¯
        negative_aspects = []
        if insights_section:
            negative_elements = await insights_section.query_selector_all("a[data-csa-c-item-id*='_NEGATIVE']")
            for elem in negative_elements:
                aspect_text = await elem.inner_text()
                aspect_cleaned = re.sub(r'[^a-zA-Z\s]', '', aspect_text).strip()  # æ¸…ç†æ–‡æœ¬
                if aspect_cleaned:
                    negative_aspects.append(aspect_cleaned)

        # å¦‚æœæ˜¯é‡è¯•æˆåŠŸï¼Œæç¤ºç”¨æˆ·
        if retry_count > 0:
            logging.info(f"ğŸ”„ ASIN {asin} é‡è¯•æˆåŠŸï¼")
        # è®¡ç®—è€—æ—¶å¹¶è¾“å‡º
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        logging.info(f"âœ… çˆ¬å–æˆåŠŸï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’")
        # è¿”å›æ‰€æœ‰æŠ“å–åˆ°çš„æ•°æ®
        return {
            "asin": asin,
            "brand": brand,
            "brand_link": brand_link,
            "title": title,
            "price": price,
            "bought": bought,
            "fabric_type": fabric_type,
            "url": url,
            "frequently_returned": frequently_returned,
            "variants": variant_asins,
            "rating": rating,
            "review_count": review_count,
            "negative_aspects": negative_aspects,
            "customer_say": customer_say
        }
    except Exception as e:
        logging.error(f"âŒ çˆ¬å–å¤±è´¥: {asin}ï¼Œé”™è¯¯: {str(e)}")
        if retry_count < MAX_RETRIES:  # å¦‚æœæœªè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»§ç»­å°è¯•
            logging.warning(f"âš ï¸ ASIN {asin} åŠ å…¥é‡è¯•é˜Ÿåˆ—ï¼Œé‡è¯•æ¬¡æ•°: {retry_count + 1}")
            await asyncio.sleep(random.uniform(2, 5))  # éšæœºå»¶è¿Ÿåé‡è¯•
            return await get_product_details(asin, page, retry_count + 1)
        else:
            logging.error(f"ğŸš¨ ASIN {asin} é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œæ”¾å¼ƒçˆ¬å–")
            return None  # é‡è¯•å¤±è´¥ï¼Œè¿”å› None

# æµ‹è¯•å‡½æ•°ï¼Œç”¨äºå•ä¸ª ASIN çš„æŠ“å–å’Œè°ƒè¯•
async def test_scraper():
    """
    æµ‹è¯•æŠ“å–åŠŸèƒ½ï¼Œä»å•ä¸ª ASIN å¼€å§‹ï¼Œé€’å½’æŠ“å–å…¶å˜ä½“ï¼Œå¹¶è¾“å‡ºç»“æœã€‚
    """
    test_asin = "B0CN8SL6MV"  # æµ‹è¯•ç”¨çš„åˆå§‹ ASIN
    scraped_data = {}  # å­˜å‚¨æŠ“å–ç»“æœ
    to_scrape = [test_asin]  # å¾…æŠ“å–çš„ ASIN é˜Ÿåˆ—
    seen_asins = set()  # è®°å½•å·²å¤„ç†çš„ ASIN
    start_time = time.perf_counter()  # è®°å½•å¼€å§‹æ—¶é—´

    # ä½¿ç”¨ Playwright å¯åŠ¨æµè§ˆå™¨
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  # æ— å¤´æ¨¡å¼å¯åŠ¨ Chromium
        context = await browser.new_context()  # åˆ›å»ºæ–°çš„æµè§ˆä¸Šä¸‹æ–‡
        try:
            # åŠ è½½ Cookies ä»¥æ¨¡æ‹Ÿç™»å½•çŠ¶æ€
            with open(COOKIES_FILE, "r") as f:
                cookies = json.load(f)
                await context.add_cookies(cookies)
                logging.info("âœ… å·²åŠ è½½ Amazon ç™»å½• Cookies")
        except:
            logging.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ° Cookiesï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œ `login.py` æ‰‹åŠ¨ç™»å½•")
            await browser.close()
            return
        page = await context.new_page()  # åˆ›å»ºæ–°é¡µé¢

        # å¾ªç¯å¤„ç†å¾…æŠ“å–çš„ ASIN
        while to_scrape:
            current_asin = to_scrape.pop(0)  # ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸€ä¸ª ASIN
            if current_asin in seen_asins:  # å¦‚æœå·²å¤„ç†ï¼Œè·³è¿‡
                continue
            seen_asins.add(current_asin)  # æ ‡è®°ä¸ºå·²å¤„ç†
            product_info = await get_product_details(current_asin, page)  # æŠ“å–è¯¦æƒ…
            if product_info:  # å¦‚æœæŠ“å–æˆåŠŸ
                scraped_data[current_asin] = product_info  # ä¿å­˜ç»“æœ
                # å¦‚æœæœ‰å˜ä½“ ASINï¼ŒåŠ å…¥å¾…æŠ“å–é˜Ÿåˆ—
                if "variants" in product_info:
                    for variant_asin in product_info["variants"]:
                        if variant_asin not in seen_asins and variant_asin not in to_scrape:
                            to_scrape.append(variant_asin)
        await page.close()  # å…³é—­é¡µé¢
        await browser.close()  # å…³é—­æµè§ˆå™¨

        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.perf_counter()
        total_time = end_time - start_time
        # å°†ç»“æœä¿å­˜ä¸º CSV
        df = pd.DataFrame(scraped_data.values())
        df.to_csv(OUTPUT_FILE, index=False)
        logging.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {OUTPUT_FILE}")

        # æ‰“å°æ‰€æœ‰æŠ“å–åˆ°çš„æ•°æ®ï¼Œæ–¹ä¾¿è°ƒè¯•
        logging.info("ğŸ›’ çˆ¬å–å®Œæˆï¼æ‰€æœ‰æ•°æ®å¦‚ä¸‹ï¼š")
        for asin, data in scraped_data.items():
            logging.info("=" * 50)
            logging.info(f"ASIN: {asin}")
            for key, value in data.items():
                if key != "asin":  # ASIN å·²å•ç‹¬æ‰“å°ï¼Œé¿å…é‡å¤
                    logging.info(f"{key}: {value}")
        logging.info("=" * 50)
        logging.info(f"â±ï¸ æ€»çˆ¬å–æ—¶é—´: {total_time:.2f} ç§’")
        logging.info("=" * 50)

# ç¨‹åºå…¥å£ï¼Œè¿è¡Œæµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    asyncio.run(test_scraper())  # å¯åŠ¨å¼‚æ­¥æµ‹è¯•æµç¨‹