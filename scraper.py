import asyncio
from playwright.async_api import async_playwright
import json
import random
import pandas as pd
import time  # ç”¨äºç»Ÿè®¡æ—¶é—´
import re  # å¼•å…¥æ­£åˆ™åº“

COOKIES_FILE = "amazon_cookies.json"
OUTPUT_FILE = "amazon_products.csv"
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°


async def get_variants_asins(page):
    """ è·å–å•†å“çš„å˜ä½“ ASIN åˆ—è¡¨ """
    variant_asins = set()
    variant_elements = await page.query_selector_all("li[data-asin], div[data-defaultasin], div[data-csa-c-asin]")

    for elem in variant_elements:
        asin = await elem.get_attribute("data-asin") or await elem.get_attribute("data-defaultasin") or await elem.get_attribute("data-csa-c-asin")
        if asin:
            variant_asins.add(asin.strip())

    return list(variant_asins)


async def get_product_details(asin, page, retry_count=0):
    """çˆ¬å–å•†å“è¯¦æƒ…"""
    url = f"https://www.amazon.com/dp/{asin}"
    print(f"ğŸ“¦ æ­£åœ¨çˆ¬å–å•†å“è¯¦æƒ…: {url}")

    try:
        # éšæœºå»¶è¿Ÿï¼Œå‡å°‘é£æ§ï¼ˆä¿æŒèŒƒå›´è¾ƒå°ï¼‰
        await asyncio.sleep(random.uniform(0.2, 0.8))

        # æ¢å¤ domcontentloadedï¼Œåªç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
        await page.goto(url, timeout=30000, wait_until="domcontentloaded")
        await page.wait_for_selector("#productTitle", timeout=30000)

        # è·å–å•†å“æ ‡é¢˜
        title_element = await page.query_selector("#productTitle")
        title = await title_element.inner_text() if title_element else "Title not found"

        # è·å–å“ç‰Œï¼ˆBrandï¼‰å’Œå“ç‰Œé“¾æ¥
        brand_element = await page.query_selector("#bylineInfo")
        brand = await brand_element.inner_text() if brand_element else "Brand not found"
        brand_link = await brand_element.get_attribute("href") if brand_element else None

        # **å¤„ç†å“ç‰Œå­—æ®µ**
        if "Visit the" in brand and "Store" in brand:
            brand = brand.replace("Visit the", "").replace("Store", "").strip()
        elif "Brand:" in brand:
            brand = brand.replace("Brand:", "").strip()
        brand = re.sub(r'[^a-zA-Z0-9\s-]', '', brand).strip()

        if brand_link and not brand_link.startswith("http"):
            brand_link = f"https://www.amazon.com{brand_link}"

        # è·å–ä»·æ ¼
        price = "Price not found"
        price_element = await page.query_selector("span.a-offscreen")
        if price_element:
            price_text = await price_element.inner_text()
            if price_text.strip():
                price = price_text.strip()

        if price == "Price not found":
            price_whole_element = await page.query_selector("span.a-price-whole")
            price_fraction_element = await page.query_selector("span.a-price-fraction")
            whole_text = (await price_whole_element.inner_text()).strip() if price_whole_element else ""
            fraction_text = (await price_fraction_element.inner_text()).strip() if price_fraction_element else ""
            whole_text = whole_text.replace("\n", "").replace(" ", "").replace(".", "")
            fraction_text = fraction_text.replace("\n", "").replace(" ", "")
            if whole_text and fraction_text:
                price = f"${whole_text}.{fraction_text}"
            elif whole_text:
                price = f"${whole_text}"

        # è·å–ä¸Šæœˆé”€é‡
        bought_element = await page.query_selector("#social-proofing-faceout-title-tk_bought .a-text-bold")
        bought = (await bought_element.inner_text()).split()[0] if bought_element else "< 50"

        # è·å– fabric_type
        fabric_type = None
        details_section = await page.query_selector("#productFactsDesktopExpander")
        if details_section:
            fabric_element = await details_section.query_selector("span.a-color-base:has-text('Fabric type')")
            if fabric_element:
                fabric_type_element = await fabric_element.evaluate_handle(
                    "el => el.parentElement.parentElement.nextElementSibling.querySelector('.a-color-base')"
                )
                fabric_type = await fabric_type_element.inner_text() if fabric_type_element else None

        # `Frequently returned item` æ ‡ç­¾
        frequently_returned_element = await page.query_selector(
            "div#buyingOptionNostosBadge_feature_div .hrrv-badge-T2-title p span.a-text-bold"
        )
        frequently_returned = True if frequently_returned_element else False

        # è·å–å˜ä½“ ASIN
        variant_asins = await get_variants_asins(page)

        # è·å–è¯„åˆ† (Rating) å’Œè¯„åˆ†æ•°é‡ (Review Count)
        rating = "Rating not found"
        review_count = "Review count not found"
        rating_element = await page.query_selector("#averageCustomerReviews .a-icon-alt")
        review_count_element = await page.query_selector("#acrCustomerReviewText")

        if rating_element:
            rating_text = await rating_element.inner_text()
            rating_match = re.search(r"(\d+\.\d+|\d+)", rating_text)
            rating = rating_match.group(0) if rating_match else "Rating not found"

        if review_count_element:
            review_text = await review_count_element.inner_text()
            review_match = re.search(r"(\d+,?\d*)", review_text)
            review_count = review_match.group(0).replace(",", "") if review_match else "Review count not found"

        # è·å– Date First Availableï¼ˆä¼˜åŒ–ä¸ºå•æ¬¡æŸ¥è¯¢ + ç¼“å­˜é€‰æ‹©å™¨ï¼‰
        date_first_available = "Date not found"
        date_selectors = [
            "#detailBullets_feature_div li span:has-text('Date First Available')",
            "#productDetails_detailBullets_sections1 tr:has-text('Date First Available') td",
            "#productDetails_techSpec_section_1 tr:has-text('Date First Available') td",
            "th.prodDetSectionEntry:has-text('Date First Available') + td"
        ]

        # å•æ¬¡æŸ¥è¯¢æ‰€æœ‰å¯èƒ½çš„é€‰æ‹©å™¨ï¼Œå‡å°‘å¾ªç¯å¼€é”€
        for selector in date_selectors:
            date_element = await page.query_selector(selector)
            if date_element:
                date_value = await date_element.inner_text()
                date_match = re.search(r"(?:\w+\s+\d{1,2},\s+\d{4}|\d{1,2}\s+\w+\s+\d{4})", date_value)
                if date_match:
                    date_first_available = date_match.group(0).strip()
                    # åªåœ¨æˆåŠŸæ—¶æ‰“å°æ—¥å¿—ï¼Œå‡å°‘ I/O
                    # print(f"â„¹ï¸ æ‰¾åˆ° Date First Available: {date_first_available} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                    break
                # å¦‚æœè°ƒè¯•éœ€è¦ï¼Œå¯å–æ¶ˆæ³¨é‡Š
                # else:
                #     print(f"â„¹ï¸ æå–å€¼éæ—¥æœŸæ ¼å¼: {date_value} (é€‰æ‹©å™¨: {selector})")
            # å¦‚æœè°ƒè¯•éœ€è¦ï¼Œå¯å–æ¶ˆæ³¨é‡Š
            # else:
            #     print(f"â„¹ï¸ æœªæ‰¾åˆ° Date First Available (é€‰æ‹©å™¨: {selector})")

        if retry_count > 0:
            print(f"ğŸ”„ ASIN {asin} é‡è¯•æˆåŠŸï¼")
        print(f"âœ… çˆ¬å–æˆåŠŸ")

        return {
            "asin": asin,
            "brand": brand,
            "brand_link": brand_link,
            "title": title,
            "price": price,
            "bought": bought,
            "fabric_type": fabric_type,
            "url": url,
            "frequently_returned": frequently_returned,
            "variants": variant_asins,
            "rating": rating,
            "review_count": review_count,
            "date_first_available": date_first_available
        }

    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {asin}ï¼Œé”™è¯¯: {e}")
        if retry_count < MAX_RETRIES:
            print(f"âš ï¸ ASIN {asin} åŠ å…¥é‡è¯•é˜Ÿåˆ—ï¼Œé‡è¯•æ¬¡æ•°: {retry_count + 1}")
            await asyncio.sleep(random.uniform(1, 3))
            return await get_product_details(asin, page, retry_count + 1)
        else:
            print(f"ğŸš¨ ASIN {asin} é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œæ”¾å¼ƒçˆ¬å–")
            return {"asin": asin, "retry": True}


async def test_scraper():
    """ æµ‹è¯•çˆ¬å–å•ä¸ª ASINï¼Œå¹¶é€’å½’çˆ¬å–æ‰€æœ‰å˜ä½“ """
    test_asin = "B0C61QXH6F"
    scraped_data = {}
    to_scrape = [test_asin]
    seen_asins = set()

    start_time = time.perf_counter()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()

        try:
            with open(COOKIES_FILE, "r") as f:
                cookies = json.load(f)
                await context.add_cookies(cookies)
                print("âœ… å·²åŠ è½½ Amazon ç™»å½• Cookies")
        except:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° Cookiesï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œ `login.py` æ‰‹åŠ¨ç™»å½•")
            await browser.close()
            return

        page = await context.new_page()

        while to_scrape:
            current_asin = to_scrape.pop(0)
            if current_asin in seen_asins:
                continue

            seen_asins.add(current_asin)
            product_info = await get_product_details(current_asin, page)

            if product_info:
                if product_info.get("retry"):
                    to_scrape.append(current_asin)
                else:
                    scraped_data[current_asin] = product_info
                    if "variants" in product_info:
                        for variant_asin in product_info["variants"]:
                            if variant_asin not in seen_asins and variant_asin not in to_scrape:
                                to_scrape.append(variant_asin)

        await page.close()
        await browser.close()

        end_time = time.perf_counter()
        total_time = end_time - start_time

        df = pd.DataFrame(scraped_data.values())
        df.to_csv(OUTPUT_FILE, index=False)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {OUTPUT_FILE}")

        print("\nğŸ›’ çˆ¬å–å®Œæˆï¼æ‰€æœ‰æ•°æ®å¦‚ä¸‹ï¼š")
        for asin, data in scraped_data.items():
            print("=" * 50)
            print(f"ASIN: {asin}")
            for key, value in data.items():
                if key != "asin":
                    print(f"{key}: {value}")

        print("=" * 50)
        print(f"â±ï¸ æ€»çˆ¬å–æ—¶é—´: {total_time:.2f} ç§’")
        print("=" * 50)


if __name__ == "__main__":
    asyncio.run(test_scraper())