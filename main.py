import asyncio
import csv
import json
from search import search_products
from scraper import get_product_details
from playwright.async_api import async_playwright

# é…ç½®
CONFIG_FILE = "config.json"

# è¯»å–é…ç½®
with open(CONFIG_FILE, "r") as f:
    config = json.load(f)

SEARCH_QUERY = config["search_query"]
CSV_FILE = config["csv_file"]
OUTPUT_FILE = config["output_file"]
MAX_WORKERS = config["max_processes"]
MAX_PAGES = config["max_pages"]
COOKIES_FILE = config["cookies_file"]


async def worker(queue, context, results):
    """ä»»åŠ¡é˜Ÿåˆ— Workerï¼šä»é˜Ÿåˆ—è·å– ASIN å¹¶çˆ¬å–"""
    while not queue.empty():
        asin = await queue.get()
        print(f"ğŸ›’ ä»»åŠ¡é˜Ÿåˆ—é¢†å– ASIN: {asin}")
        page = await context.new_page()
        product_data = await get_product_details(asin, page)
        await page.close()
        queue.task_done()  # **æ ‡è®°ä»»åŠ¡å·²å®Œæˆ**
        if product_data:
            results.append(product_data)  # **å­˜å‚¨ç»“æœ**


async def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºä»»åŠ¡é˜Ÿåˆ— + å¹¶è¡Œçˆ¬å–"""
    asins = await search_products(SEARCH_QUERY, CSV_FILE, MAX_PAGES)

    if not asins:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° ASINï¼Œé€€å‡ºç¨‹åºï¼")
        return

    queue = asyncio.Queue()

    # æ·»åŠ æ‰€æœ‰ ASIN åˆ°é˜Ÿåˆ—
    for asin in asins:
        await queue.put(asin)

    # åˆ›å»º Playwright æµè§ˆå™¨
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()

        # **åŠ è½½ Amazon ç™»å½• Cookies**
        try:
            with open(COOKIES_FILE, "r") as f:
                cookies = json.load(f)
                await context.add_cookies(cookies)
                print("âœ… å·²åŠ è½½ Amazon ç™»å½• Cookies")
        except:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° Cookiesï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œ `login.py` æ‰‹åŠ¨ç™»å½•")
            await browser.close()
            return

        # å­˜å‚¨çˆ¬å–ç»“æœ
        results = []

        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—çš„ Workersï¼ˆ**åˆ›å»ºå’Œ ASIN æ•°é‡ç›¸åŒçš„ä»»åŠ¡**ï¼‰
        tasks = [worker(queue, context, results)
                 for _ in range(min(len(asins), MAX_WORKERS))]

        await asyncio.gather(*tasks)  # **ç¡®ä¿æ‰€æœ‰ä»»åŠ¡éƒ½æ‰§è¡Œå®Œ**

        # å…³é—­æµè§ˆå™¨
        await browser.close()

    # å­˜å…¥ CSV æ–‡ä»¶
    with open(SEARCH_QUERY + OUTPUT_FILE, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["ASIN", "Title", "Price",
                        "URL", "Bought", "Frequently Returned"])

        for product_data in results:
            writer.writerow([product_data["asin"], product_data["title"], product_data["price"],
                             product_data["url"], product_data["bought"], product_data["frequently_returned"]])
            print(f"âœ… å·²å­˜å…¥ CSV: {product_data['title']}")

    print(f"\nğŸ‰ æ‰€æœ‰å•†å“ä¿¡æ¯å·²ä¿å­˜åˆ° `{SEARCH_QUERY + OUTPUT_FILE}`ï¼")

# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    asyncio.run(main())
