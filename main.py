import asyncio
import csv
import json
from search import search_products  # å¯¼å…¥æœç´¢æ¨¡å—ï¼Œè·å– ASIN åˆ—è¡¨
from scraper import get_product_details  # å¯¼å…¥æŠ“å–æ¨¡å—ï¼Œè·å–å•†å“è¯¦æƒ…
from playwright.async_api import async_playwright  # å¯¼å…¥ Playwright çš„å¼‚æ­¥ API
import time
import os
from datetime import datetime  # æ–°å¢æ—¶é—´æ¨¡å—

# é…ç½®æ–‡ä»¶çš„è·¯å¾„
CONFIG_FILE = "config.json"
# CSV æ–‡ä»¶ä¿å­˜ç›®å½•
CSV_DIR = "csv"  # ç›´æ¥å®šä¹‰ï¼Œä¸æ£€æŸ¥å’Œåˆ›å»º

# è¯»å–é…ç½®æ–‡ä»¶ï¼Œè·å–å…¨å±€å‚æ•°
with open(CONFIG_FILE, "r") as f:
    config = json.load(f)

# ä»é…ç½®æ–‡ä»¶ä¸­æå–å‚æ•°
SEARCH_QUERIES = config["search_query"]  # æœç´¢å…³é”®è¯åˆ—è¡¨ï¼Œä¾‹å¦‚ ["toilet paper holder", "vintage apron"]
CSV_FILE_BASE = config["csv_file"]  # ä¿å­˜ ASIN åˆ—è¡¨çš„ CSV æ–‡ä»¶åŸºç¡€å
OUTPUT_FILE_BASE = config["output_file"]  # ä¿å­˜æœ€ç»ˆå•†å“æ•°æ®çš„ CSV æ–‡ä»¶åŸºç¡€å
MAX_WORKERS = config["max_processes"]  # æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°
MAX_PAGES = config["max_pages"]  # æœç´¢ç»“æœçš„æœ€å¤§ç¿»é¡µæ•°
COOKIES_FILE = config["cookies_file"]  # Cookies æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ¨¡æ‹Ÿç™»å½•

# å®šä¹‰å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼Œè´Ÿè´£ä»é˜Ÿåˆ—ä¸­è·å– ASIN å¹¶æŠ“å–æ•°æ®
async def worker(queue, context, results, seen_asins, failed_asins):
    """
    å¼‚æ­¥å·¥ä½œè¿›ç¨‹ï¼Œä»ä»»åŠ¡é˜Ÿåˆ—ä¸­è·å– ASIN å¹¶è°ƒç”¨ get_product_details æŠ“å–å•†å“ä¿¡æ¯ã€‚
    
    :param queue: asyncio.Queueï¼Œå­˜å‚¨å¾…å¤„ç†çš„ ASIN
    :param context: Playwright æµè§ˆä¸Šä¸‹æ–‡ï¼Œç”¨äºåˆ›å»ºæ–°é¡µé¢
    :param results: listï¼Œå­˜å‚¨æŠ“å–ç»“æœ
    :param seen_asins: setï¼Œè®°å½•å·²å¤„ç†çš„ ASINï¼Œé¿å…é‡å¤
    :param failed_asins: setï¼Œè®°å½•å¤±è´¥çš„ ASIN
    """
    while not queue.empty():  # å½“é˜Ÿåˆ—ä¸ä¸ºç©ºæ—¶æŒç»­å¤„ç†
        asin = await queue.get()  # ä»é˜Ÿåˆ—ä¸­è·å–ä¸€ä¸ª ASIN
        if asin in seen_asins:  # å¦‚æœ ASIN å·²å¤„ç†è¿‡ï¼Œè·³è¿‡
            queue.task_done()  # æ ‡è®°ä»»åŠ¡å®Œæˆ
            continue
        print(f"ğŸ›’ ä»»åŠ¡é˜Ÿåˆ—é¢†å– ASIN: {asin}")  # æ˜¾ç¤ºå½“å‰å¤„ç†çš„ ASIN
        page = await context.new_page()  # åˆ›å»ºæ–°é¡µé¢
        # æ‹¦æˆªå¹¶ç¦ç”¨ä¸å¿…è¦çš„èµ„æºè¯·æ±‚ï¼Œä¼˜åŒ–åŠ è½½é€Ÿåº¦
        await page.route("**/*.{png,jpg,jpeg,gif,svg,webp,css,woff,woff2,js,mp4,webm}", lambda route: route.abort())
        product_data = await get_product_details(asin, page)  # æŠ“å–å•†å“è¯¦æƒ…
        await page.close()  # å…³é—­é¡µé¢ï¼Œé‡Šæ”¾èµ„æº
        queue.task_done()  # æ ‡è®°ä»»åŠ¡å®Œæˆ
        if product_data:  # å¦‚æœæˆåŠŸæŠ“å–åˆ°æ•°æ®
            seen_asins.add(asin)  # å°† ASIN æ ‡è®°ä¸ºå·²å¤„ç†
            results.append(product_data)  # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        else:  # å¦‚æœæŠ“å–å¤±è´¥
            failed_asins.add(asin)  # è®°å½•å¤±è´¥çš„ ASIN

async def process_query(query, csv_file_base, output_file_base):
    """å¤„ç†å•ä¸ªæœç´¢è¯çš„çˆ¬å–æµç¨‹"""
    timestamp = datetime.now().strftime("%Y%m%d%H%M")  # ç”Ÿæˆæ—¶é—´æˆ³ï¼Œå¦‚ 202503011430
    csv_file_path = os.path.join(CSV_DIR, f"{query}_{timestamp}_{csv_file_base}")
    output_file_path = os.path.join(CSV_DIR, f"{query}_{timestamp}_{output_file_base}")
    
    # è°ƒç”¨ search_products è·å– ASIN åˆ—è¡¨
    asins = await search_products(query, csv_file_path, MAX_PAGES)
    if not asins:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ° ASINï¼Œè·³è¿‡
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ° ASIN for '{query}'ï¼Œè·³è¿‡ï¼")
        return

    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—ï¼Œç”¨äºåˆ†å‘ ASIN ç»™å·¥ä½œè¿›ç¨‹
    queue = asyncio.Queue()
    seen_asins = set()  # æˆåŠŸæŠ“å–çš„ ASIN
    failed_asins = set()  # å¤±è´¥çš„ ASIN
    # å°†æ‰€æœ‰ ASIN æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­
    for asin in asins:
        await queue.put(asin)

    # ä½¿ç”¨ Playwright å¯åŠ¨æµè§ˆå™¨
    async with async_playwright() as p:
        # å¯åŠ¨ Chromium æµè§ˆå™¨ï¼Œæ— å¤´æ¨¡å¼ï¼Œå¸¦ä¼˜åŒ–å‚æ•°
        browser = await p.chromium.launch(
            headless=True,
            args=["--disable-gpu", "--disable-web-security", "--disable-dev-shm-usage", "--no-sandbox"]
        )
        context = await browser.new_context()  # åˆ›å»ºæ–°çš„æµè§ˆä¸Šä¸‹æ–‡

        # åŠ è½½ Amazon ç™»å½• Cookies
        try:
            with open(COOKIES_FILE, "r") as f:
                cookies = json.load(f)
                await context.add_cookies(cookies)  # å°† Cookies æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
                print(f"âœ… å·²åŠ è½½ Amazon ç™»å½• Cookies for '{query}'")
        except:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° Cookiesï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œ `login.py` æ‰‹åŠ¨ç™»å½•")
            await browser.close()
            return

        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        results = []
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡ï¼Œæ•°é‡ä¸º ASIN æ€»æ•°å’Œ MAX_WORKERS çš„è¾ƒå°å€¼
        tasks = [worker(queue, context, results, seen_asins, failed_asins) for _ in range(min(len(asins), MAX_WORKERS))]
        await asyncio.gather(*tasks)  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await browser.close()  # å…³é—­æµè§ˆå™¨

        # å¦‚æœæ²¡æœ‰æŠ“å–åˆ°æ•°æ®ï¼Œæç¤ºå¹¶è¿”å›
        if not results:
            print(f"âŒ æ²¡æœ‰çˆ¬å–åˆ°æ•°æ® for '{query}'")
            return

        # ä»ç¬¬ä¸€ä¸ªç»“æœåŠ¨æ€è·å–å­—æ®µå
        fieldnames = list(results[0].keys())
        # å°†ç»“æœå†™å…¥ CSV æ–‡ä»¶
        with open(output_file_path, "w", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            # æ’é™¤ url å’Œ brand_link å­—æ®µï¼Œç”Ÿæˆè¡¨å¤´
            field_names = [field for field in fieldnames if field not in ["url", "brand_link"]]
            writer.writerow(field_names)  # å†™å…¥è¡¨å¤´
            # éå†æ‰€æœ‰æŠ“å–ç»“æœ
            for product_data in results:
                # å°† ASIN è½¬æ¢ä¸ºè¶…é“¾æ¥æ ¼å¼
                product_data["asin"] = f'=HYPERLINK("https://www.amazon.com/dp/{product_data["asin"]}", "{product_data["asin"]}")'
                # å¦‚æœæœ‰å“ç‰Œé“¾æ¥ï¼Œå°†å“ç‰Œè½¬æ¢ä¸ºè¶…é“¾æ¥
                if product_data.get("brand_link"):
                    product_data["brand"] = f'=HYPERLINK("{product_data["brand_link"]}", "{product_data.get("brand", "N/A")}")'
                # å†™å…¥ä¸€è¡Œæ•°æ®ï¼Œä½¿ç”¨ get æ–¹æ³•é¿å…å­—æ®µç¼ºå¤±
                writer.writerow([product_data.get(field, "N/A") for field in field_names])

        # è¾“å‡ºæŠ“å–ç»Ÿè®¡ä¿¡æ¯
        total_asins = len(asins)
        successful_asins = len(results)
        failed_count = total_asins - successful_asins
        print(f"\nğŸ‰ '{query}' å•†å“ä¿¡æ¯å·²ä¿å­˜åˆ° `{output_file_path}`ï¼å…±çˆ¬å– {total_asins} ä¸ª ASINï¼ŒæˆåŠŸ {successful_asins} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªï¼Œå¤±è´¥çš„ ASIN: {list(failed_asins)}")

# å®šä¹‰ä¸»å‡½æ•°ï¼Œåè°ƒæœç´¢å’ŒæŠ“å–æµç¨‹
async def main():
    """ä¸»å‡½æ•°ï¼šè´Ÿè´£å¾ªç¯å¤„ç†æ‰€æœ‰æœç´¢è¯å¹¶è°ƒç”¨çˆ¬å–æµç¨‹"""
    for query in SEARCH_QUERIES:
        print(f"\n=== å¼€å§‹å¤„ç†æœç´¢è¯: {query} ===")
        await process_query(query, CSV_FILE_BASE, OUTPUT_FILE_BASE)

# ç¨‹åºå…¥å£ï¼Œè¿è¡Œä¸»å‡½æ•°å¹¶è®¡æ—¶
if __name__ == "__main__":
    start_time = time.perf_counter()  # è®°å½•å¼€å§‹æ—¶é—´
    asyncio.run(main())  # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    end_time = time.perf_counter()  # è®°å½•ç»“æŸæ—¶é—´
    total_time = end_time - start_time  # è®¡ç®—æ€»è€—æ—¶
    print("=" * 50)
    print(f"â³ æ•´ä¸ª `main.py` è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
    print("=" * 50)