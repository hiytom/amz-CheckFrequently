import asyncio
import csv
import json
import logging
from search import search_products  # å¯¼å…¥æœç´¢æ¨¡å—ï¼Œè·å– ASIN åˆ—è¡¨
from scraper import get_product_details  # å¯¼å…¥æŠ“å–æ¨¡å—ï¼Œè·å–å•†å“è¯¦æƒ…
from playwright.async_api import async_playwright  # å¯¼å…¥ Playwright çš„å¼‚æ­¥ API
import time
import os
from datetime import datetime  # æ–°å¢æ—¶é—´æ¨¡å—

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,  # è®¾ç½®é»˜è®¤æ—¥å¿—çº§åˆ«ä¸º INFO
    format="%(asctime)s - %(levelname)s - %(message)s",  # æ—¥å¿—æ ¼å¼ï¼šæ—¶é—´ - çº§åˆ« - æ¶ˆæ¯
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler("crawler.log", encoding="utf-8")  # è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)

# é…ç½®æ–‡ä»¶çš„è·¯å¾„
CONFIG_FILE = "config.json"
# CSV æ–‡ä»¶ä¿å­˜ç›®å½•
CSV_DIR = "csv"  # ç›´æ¥å®šä¹‰ï¼Œä¸æ£€æŸ¥å’Œåˆ›å»º

# è¯»å–é…ç½®æ–‡ä»¶ï¼Œè·å–å…¨å±€å‚æ•°
with open(CONFIG_FILE, "r") as f:
    config = json.load(f)

# ä»é…ç½®æ–‡ä»¶ä¸­æå–å‚æ•°
SEARCH_QUERIES = config["search_query"]  # æœç´¢å…³é”®è¯åˆ—è¡¨ï¼Œä¾‹å¦‚ ["toilet paper holder", "vintage apron"]
CSV_FILE_BASE = config["csv_file"]  # ä¿å­˜ ASIN åˆ—è¡¨çš„ CSV æ–‡ä»¶åŸºç¡€å
OUTPUT_FILE_BASE = config["output_file"]  # ä¿å­˜æœ€ç»ˆå•†å“æ•°æ®çš„ CSV æ–‡ä»¶åŸºç¡€å
MAX_WORKERS = config["max_processes"]  # æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°
MAX_PAGES = config["max_pages"]  # æœç´¢ç»“æœçš„æœ€å¤§ç¿»é¡µæ•°
COOKIES_FILE = config["cookies_file"]  # Cookies æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ¨¡æ‹Ÿç™»å½•

# å®šä¹‰å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼Œè´Ÿè´£ä»é˜Ÿåˆ—ä¸­è·å– ASIN å¹¶æŠ“å–æ•°æ®
async def worker(queue, context, results, seen_asins, failed_asins):
    """
    å¼‚æ­¥å·¥ä½œè¿›ç¨‹ï¼Œä»ä»»åŠ¡é˜Ÿåˆ—ä¸­è·å– ASIN å¹¶è°ƒç”¨ get_product_details æŠ“å–å•†å“ä¿¡æ¯ã€‚
    
    :param queue: asyncio.Queueï¼Œå­˜å‚¨å¾…å¤„ç†çš„ ASIN
    :param context: Playwright æµè§ˆä¸Šä¸‹æ–‡ï¼Œç”¨äºåˆ›å»ºæ–°é¡µé¢
    :param results: listï¼Œå­˜å‚¨æŠ“å–ç»“æœ
    :param seen_asins: setï¼Œè®°å½•å·²å¤„ç†çš„ ASINï¼Œé¿å…é‡å¤
    :param failed_asins: setï¼Œè®°å½•å¤±è´¥çš„ ASIN
    """
    while not queue.empty():  # å½“é˜Ÿåˆ—ä¸ä¸ºç©ºæ—¶æŒç»­å¤„ç†
        try:
            asin = await queue.get()  # ä»é˜Ÿåˆ—ä¸­è·å–ä¸€ä¸ª ASIN
        except asyncio.CancelledError:
            logging.warning("âš ï¸ è·å– ASIN æ—¶ä»»åŠ¡è¢«å–æ¶ˆ")
            queue.task_done()
            raise
        page = None  # åˆå§‹åŒ– page ä¸º None
        try:
            if asin in seen_asins:  # å¦‚æœ ASIN å·²å¤„ç†è¿‡ï¼Œè·³è¿‡
                queue.task_done()  # æ ‡è®°ä»»åŠ¡å®Œæˆ
                continue
            logging.info(f"ğŸ›’ ä»»åŠ¡é˜Ÿåˆ—é¢†å– ASIN: {asin}")  # æ˜¾ç¤ºå½“å‰å¤„ç†çš„ ASIN
            page = await context.new_page()  # åˆ›å»ºæ–°é¡µé¢
            # æ‹¦æˆªå¹¶ç¦ç”¨ä¸å¿…è¦çš„èµ„æºè¯·æ±‚ï¼Œä¼˜åŒ–åŠ è½½é€Ÿåº¦
            await page.route("**/*.{png,jpg,jpeg,gif,svg,webp,css,woff,woff2,js,mp4,webm}", lambda route: route.abort())
            product_data = await get_product_details(asin, page)  # æŠ“å–å•†å“è¯¦æƒ…
            await page.close()  # å…³é—­é¡µé¢ï¼Œé‡Šæ”¾èµ„æº
            queue.task_done()  # æ ‡è®°ä»»åŠ¡å®Œæˆ
            if product_data:  # å¦‚æœæˆåŠŸæŠ“å–åˆ°æ•°æ®
                seen_asins.add(asin)  # å°† ASIN æ ‡è®°ä¸ºå·²å¤„ç†
                results.append(product_data)  # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            else:  # å¦‚æœæŠ“å–å¤±è´¥
                failed_asins.add(asin)  # è®°å½•å¤±è´¥çš„ ASIN
        except asyncio.CancelledError:
            logging.warning(f"âš ï¸ ä»»åŠ¡å¤„ç† ASIN {asin} è¢«å–æ¶ˆ")
            if page is not None:  # æ£€æŸ¥ page æ˜¯å¦å·²åˆ›å»º
                try:
                    await page.close()
                except Exception as e:
                    logging.debug(f"å…³é—­é¡µé¢ {asin} æ—¶å‡ºé”™: {str(e)}")
            queue.task_done()
            raise

async def process_query(query, csv_file_base, output_file_base, browser, task_list):
    """å¤„ç†å•ä¸ªæœç´¢è¯çš„çˆ¬å–æµç¨‹"""
    timestamp = datetime.now().strftime("%Y%m%d%H%M")  # ç”Ÿæˆæ—¶é—´æˆ³ï¼Œå¦‚ 202503011430
    csv_file_path = os.path.join(CSV_DIR, f"{query}_{timestamp}_{csv_file_base}")
    output_file_path = os.path.join(CSV_DIR, f"{query}_{timestamp}_{output_file_base}")
    
    # è°ƒç”¨ search_products è·å– ASIN åˆ—è¡¨
    asins = await search_products(query, csv_file_path, MAX_PAGES)
    if not asins:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ° ASINï¼Œè·³è¿‡
        logging.warning(f"âŒ æ²¡æœ‰æ‰¾åˆ° ASIN for '{query}'ï¼Œè·³è¿‡ï¼")
        return

    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—ï¼Œç”¨äºåˆ†å‘ ASIN ç»™å·¥ä½œè¿›ç¨‹
    queue = asyncio.Queue()
    seen_asins = set()  # æˆåŠŸæŠ“å–çš„ ASIN
    failed_asins = set()  # å¤±è´¥çš„ ASIN
    # å°†æ‰€æœ‰ ASIN æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­
    for asin in asins:
        await queue.put(asin)

    # åˆ›å»ºæ–°çš„æµè§ˆä¸Šä¸‹æ–‡
    context = await browser.new_context()

    # åŠ è½½ Amazon ç™»å½• Cookies
    try:
        with open(COOKIES_FILE, "r") as f:
            cookies = json.load(f)
            await context.add_cookies(cookies)  # å°† Cookies æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            logging.info(f"âœ… å·²åŠ è½½ Amazon ç™»å½• Cookies for '{query}'")
    except:
        logging.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° Cookiesï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œ `login.py` æ‰‹åŠ¨ç™»å½•")
        try:
            await context.close()
        except Exception as e:
            logging.debug(f"å…³é—­ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {str(e)}")
        return

    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    results = []
    # åˆ›å»ºå¹¶è¡Œä»»åŠ¡ï¼Œæ•°é‡ä¸º ASIN æ€»æ•°å’Œ MAX_WORKERS çš„è¾ƒå°å€¼
    tasks = [asyncio.create_task(worker(queue, context, results, seen_asins, failed_asins)) 
             for _ in range(min(len(asins), MAX_WORKERS))]
    task_list.extend(tasks)  # å°†ä»»åŠ¡æ·»åŠ åˆ°å…¨å±€ä»»åŠ¡åˆ—è¡¨ä»¥ä¾¿ä¸­æ–­æ—¶å–æ¶ˆ

    try:
        await asyncio.gather(*tasks)  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    except asyncio.CancelledError:
        logging.warning(f"âš ï¸ å¤„ç† '{query}' çš„ä»»åŠ¡è¢«å–æ¶ˆ")
        # ç¡®ä¿æ‰€æœ‰ä»»åŠ¡è¢«å–æ¶ˆå¹¶å®Œæˆ
        for task in tasks:
            if not task.done():
                task.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)
        # å…³é—­ä¸Šä¸‹æ–‡ï¼Œä¸å¤„ç†å¼‚å¸¸
        try:
            await context.close()
        except Exception:
            pass  # å¿½ç•¥å…³é—­æ—¶çš„å¼‚å¸¸
        raise

    # æ­£å¸¸å®Œæˆæ—¶å…³é—­ä¸Šä¸‹æ–‡
    try:
        await context.close()
    except Exception as e:
        logging.debug(f"å…³é—­ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {str(e)}")

    # å¦‚æœæ²¡æœ‰æŠ“å–åˆ°æ•°æ®ï¼Œæç¤ºå¹¶è¿”å›
    if not results:
        logging.warning(f"âŒ æ²¡æœ‰çˆ¬å–åˆ°æ•°æ® for '{query}'")
        return

    # ä»ç¬¬ä¸€ä¸ªç»“æœåŠ¨æ€è·å–å­—æ®µå
    fieldnames = list(results[0].keys())
    # å°†ç»“æœå†™å…¥ CSV æ–‡ä»¶
    with open(output_file_path, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        # æ’é™¤ url å’Œ brand_link å­—æ®µï¼Œç”Ÿæˆè¡¨å¤´
        field_names = [field for field in fieldnames if field not in ["url", "brand_link"]]
        writer.writerow(field_names)  # å†™å…¥è¡¨å¤´
        # éå†æ‰€æœ‰æŠ“å–ç»“æœ
        for product_data in results:
            # å°† ASIN è½¬æ¢ä¸ºè¶…é“¾æ¥æ ¼å¼
            product_data["asin"] = f'=HYPERLINK("https://www.amazon.com/dp/{product_data["asin"]}", "{product_data["asin"]}")'
            # å¦‚æœæœ‰å“ç‰Œé“¾æ¥ï¼Œå°†å“ç‰Œè½¬æ¢ä¸ºè¶…é“¾æ¥
            if product_data.get("brand_link"):
                product_data["brand"] = f'=HYPERLINK("{product_data["brand_link"]}", "{product_data.get("brand", "N/A")}")'
            # å†™å…¥ä¸€è¡Œæ•°æ®ï¼Œä½¿ç”¨ get æ–¹æ³•é¿å…å­—æ®µç¼ºå¤±
            writer.writerow([product_data.get(field, "N/A") for field in field_names])

    # è¾“å‡ºæŠ“å–ç»Ÿè®¡ä¿¡æ¯
    total_asins = len(asins)
    successful_asins = len(results)
    failed_count = total_asins - successful_asins
    logging.info(f"ğŸ‰ '{query}' å•†å“ä¿¡æ¯å·²ä¿å­˜åˆ° `{output_file_path}`ï¼å…±çˆ¬å– {total_asins} ä¸ª ASINï¼ŒæˆåŠŸ {successful_asins} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªï¼Œå¤±è´¥çš„ ASIN: {list(failed_asins)}")

# å®šä¹‰ä¸»å‡½æ•°ï¼Œåè°ƒæœç´¢å’ŒæŠ“å–æµç¨‹
async def main():
    """ä¸»å‡½æ•°ï¼šè´Ÿè´£å¾ªç¯å¤„ç†æ‰€æœ‰æœç´¢è¯å¹¶è°ƒç”¨çˆ¬å–æµç¨‹"""
    task_list = []  # å­˜å‚¨æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡ä»¥ä¾¿ä¸­æ–­æ—¶å–æ¶ˆ
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=["--disable-gpu", "--disable-web-security", "--disable-dev-shm-usage", "--no-sandbox"]
        )
        try:
            for query in SEARCH_QUERIES:
                logging.info(f"=== å¼€å§‹å¤„ç†æœç´¢è¯: {query} ===")
                await process_query(query, CSV_FILE_BASE, OUTPUT_FILE_BASE, browser, task_list)
        except KeyboardInterrupt:
            logging.warning("ç”¨æˆ·ä¸­æ–­ç¨‹åºï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
            # å–æ¶ˆæ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
            for task in task_list:
                if not task.done():
                    task.cancel()
            # ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶å¿½ç•¥å¼‚å¸¸
            await asyncio.gather(*task_list, return_exceptions=True)
            # å…³é—­æµè§ˆå™¨ï¼Œä¸å¤„ç†å¼‚å¸¸
            try:
                await browser.close()
            except Exception:
                pass  # å¿½ç•¥å…³é—­æ—¶çš„å¼‚å¸¸
            logging.info("ç¨‹åºå·²ä¼˜é›…é€€å‡º")
            # æ¸…ç†äº‹ä»¶å¾ªç¯ä¸­çš„æœªå®Œæˆä»»åŠ¡
            loop = asyncio.get_running_loop()
            pending = asyncio.all_tasks(loop)
            for task in pending:
                if not task.done():
                    task.cancel()
            await asyncio.gather(*pending, return_exceptions=True)
            return
        finally:
            # ç¡®ä¿æµè§ˆå™¨åœ¨æ­£å¸¸å®Œæˆæ—¶ä¹Ÿè¢«å…³é—­
            try:
                await browser.close()
            except Exception as e:
                logging.debug(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {str(e)}")

# ç¨‹åºå…¥å£ï¼Œè¿è¡Œä¸»å‡½æ•°å¹¶è®¡æ—¶
if __name__ == "__main__":
    start_time = time.perf_counter()  # è®°å½•å¼€å§‹æ—¶é—´
    try:
        asyncio.run(main())  # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    except KeyboardInterrupt:
        logging.warning("ç”¨æˆ·ä¸­æ–­ç¨‹åºï¼Œç¨‹åºå·²é€€å‡º")
    finally:
        end_time = time.perf_counter()  # è®°å½•ç»“æŸæ—¶é—´
        total_time = end_time - start_time  # è®¡ç®—æ€»è€—æ—¶
        logging.info("=" * 50)
        logging.info(f"æ•´ä¸ª `main.py` è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
        logging.info("=" * 50)