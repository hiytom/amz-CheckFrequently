import asyncio
import csv
import json
from search import search_products
from scraper import get_product_details
from playwright.async_api import async_playwright
import time  # ç”¨äºç»Ÿè®¡æ—¶é—´

# é…ç½®
CONFIG_FILE = "config.json"

# è¯»å–é…ç½®
with open(CONFIG_FILE, "r") as f:
    config = json.load(f)

SEARCH_QUERY = config["search_query"]
CSV_FILE = config["csv_file"]
OUTPUT_FILE = config["output_file"]
MAX_WORKERS = config["max_processes"]
MAX_PAGES = config["max_pages"]
COOKIES_FILE = config["cookies_file"]


async def worker(queue, context, results):
    """ä»»åŠ¡é˜Ÿåˆ— Worker: ä»é˜Ÿåˆ—è·å– ASIN å¹¶çˆ¬å–"""
    while not queue.empty():
        asin = await queue.get()
        print(f"ğŸ›’ ä»»åŠ¡é˜Ÿåˆ—é¢†å– ASIN: {asin}")
        page = await context.new_page()
        product_data = await get_product_details(asin, page)
        await page.close()
        queue.task_done()  # **æ ‡è®°ä»»åŠ¡å·²å®Œæˆ**
        if product_data:
            results.append(product_data)  # **å­˜å‚¨ç»“æœ**


async def main():
    """ä¸»å‡½æ•°ï¼šåˆ›å»ºä»»åŠ¡é˜Ÿåˆ— + å¹¶è¡Œçˆ¬å–"""
    asins = await search_products(SEARCH_QUERY, CSV_FILE, MAX_PAGES)

    if not asins:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° ASINï¼Œé€€å‡ºç¨‹åºï¼")
        return

    queue = asyncio.Queue()

    # æ·»åŠ æ‰€æœ‰ ASIN åˆ°é˜Ÿåˆ—
    for asin in asins:
        await queue.put(asin)

    # åˆ›å»º Playwright æµè§ˆå™¨
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()

        # **åŠ è½½ Amazon ç™»å½• Cookies**
        try:
            with open(COOKIES_FILE, "r") as f:
                cookies = json.load(f)
                await context.add_cookies(cookies)
                print("âœ… å·²åŠ è½½ Amazon ç™»å½• Cookies")
        except:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° Cookiesï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œ `login.py` æ‰‹åŠ¨ç™»å½•")
            await browser.close()
            return

        # å­˜å‚¨çˆ¬å–ç»“æœ
        results = []

        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—çš„ Workersï¼ˆ**åˆ›å»ºå’Œ ASIN æ•°é‡ç›¸åŒçš„ä»»åŠ¡**ï¼‰
        tasks = [worker(queue, context, results)
                 for _ in range(min(len(asins), MAX_WORKERS))]
        await asyncio.gather(*tasks)  # **ç¡®ä¿æ‰€æœ‰ä»»åŠ¡éƒ½æ‰§è¡Œå®Œ**

        # å…³é—­æµè§ˆå™¨
        await browser.close()

    # **âœ… åŠ¨æ€æå–å­—æ®µï¼Œä¸å†™æ­»**
    if not results:
        print("âŒ æ²¡æœ‰çˆ¬å–åˆ°æ•°æ®")
        return

    # **è·å– CSV å¤´éƒ¨ï¼ˆä»ç¬¬ä¸€ä¸ªå•†å“çš„æ•°æ®åŠ¨æ€æå–ï¼‰**
    fieldnames = list(results[0].keys())

    # **å­˜å…¥ CSV æ–‡ä»¶**
    with open(SEARCH_QUERY + OUTPUT_FILE, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)

        # è¯»å–å­—æ®µåˆ—è¡¨ï¼Œè·³è¿‡ `url` å’Œ `brand_link` å­—æ®µ
        field_names = [field for field in results[0].keys() if field not in [
            "url", "brand_link"]]

        # å†™å…¥è¡¨å¤´
        writer.writerow(field_names)

        for product_data in results:
            # å°† ASIN è½¬æ¢ä¸ºè¶…é“¾æ¥
            product_data["asin"] = f'=HYPERLINK("https://www.amazon.com/dp/{product_data["asin"]}", "{product_data["asin"]}")'

            # å°†å“ç‰Œé“¾æ¥è½¬æ¢ä¸ºè¶…é“¾æ¥
            if product_data.get("brand_link"):
                product_data["brand"] = f'=HYPERLINK("{product_data["brand_link"]}", "{product_data["brand"]}")'

            # å†™å…¥æ•°æ®ï¼Œè·³è¿‡ `url` å’Œ `brand_link` å­—æ®µ
            writer.writerow([product_data[field] for field in field_names])

    print(f"\nğŸ‰ æ‰€æœ‰å•†å“ä¿¡æ¯å·²ä¿å­˜åˆ° `{SEARCH_QUERY + OUTPUT_FILE}`ï¼")


# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.perf_counter()

    asyncio.run(main())

    # è®°å½•ç»“æŸæ—¶é—´
    end_time = time.perf_counter()

    # è®¡ç®—å¹¶æ‰“å°æ€»è€—æ—¶
    total_time = end_time - start_time
    print("=" * 50)
    print(f"â³ æ•´ä¸ª `main.py` è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
    print("=" * 50)
